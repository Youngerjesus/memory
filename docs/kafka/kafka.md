## Kafka

#### Consumer 가 데이터 중복을 처리하지 않게 하기 위해선 어떻게 해야하는가? 

Consumer 가 데이터를 처리했다!. 라는 걸 할 수 있는 옵션으로 비동기 식으로 진행하는 Auto Commit 과 
수동적으로 Commit 하는 방법이 있다. 

자동 커밋하는 방법은 커밋하기 전에 리밸런싱이 일어난다면 데이터를 중복으로 처리해야할거니까 
데이터를 처리한 시점에 처리했다고 커밋을 날려주는 수동 커밋을 이용해야한다. 
수동 커밋을 할 때 신경써야하는 점은 데이터를 가져오고 커밋을 하는게 아니라 데이터를 온전히 데이터베이스에 반영했다는 그 시점에 수동 커밋을 해주는게 중요하다.

하지만 이 경우에도 데이터베이스에 반영을 했는데 커밋을 하지 못하고 셧다운이 된다면 데이터를 중복으로 처리하는 경우가 생긴다. 

Graceful Shutdown 이라고해서 커밋을 반드시 하는 방법이 있는데 이러면 또 데이터를 처리하지 않았을때도 커밋을 하게되서 손실이 발생하게 되는 경우도 있을 것 같다. 
데이터를 처리하지 않았더라면 commit 을 하지 않고. 데이터를 처리했더라면 commit 을 하는 방법

#### Producer 가 보낸 메시지가 유실되지 않도록 하기 위해선 어떻게 해야할까? 

Kafka Producer 가 성공적으로 메시지를 보냈다! 를 설정하는 옵션으로 
브로커에 있는 파티션 리더만 받고 성공했다 가 있고, 리더와 팔로우 모두 받았다가 있고, 아무도 안받았다가 있다.

이 경우에 메시지 손실을 극도로 없애기 위해서 카프카의 리더와 팔로우 모두 받도록 설정을 한다. 하지만 이 경우
프로듀서에게 오는 Latency 가 길어지기 떄문에 브로커에서 설정하는 옵션으로 팔로우가 최소한 이정도만 받아도
프로듀서에게 성공적으로 보내겠다라는 옵션이 있는 `min.insync.replicas` 옵션을 설정해놓으면 응답시간도
빠르게 유지하는게 가능하다.    

#### Producer 가 보낸 메시지를 순서대로 유지시키기 위해선 어떻게 해야할까?

특정 메시지의 순서는 파티션별로 순서가 이뤄지니까 여러 파티션에 메시지를 골고루 보내게 되면 안된다. 

특정한 파티션을 지정해서 보내도록 해야하고 브로커 이슈나 네트워크 이슈로 인해서 패킷이 손실되는 경우도 있는데
이 경우 원래 retry 를 할 때 순서가 바뀌는 경우가 생길 수 있는데 이 순서를 보장해주는 카프카 프로듀서의 
옵션이 있다. `enable.idempotence=true` 로 설정하면 되고 파티션의 리더의 리밸런싱으로 인해서 메시지가
유실되는 경우가 생길 수 있으므로 `acks=all` 로 설정을 해야한다.   